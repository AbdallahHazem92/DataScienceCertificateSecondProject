---
title: "World Happiness Report"
author: "Abdallah Hazem"
date: "5/16/2020"
output:
  pdf_document: default
  toc: yes
  toc_depth: 3
  number_sections: yes
---

```{r setup, message = F, warning = F, echo=FALSE  }
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message = F, warning = F, echo=FALSE , include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

if(!require(ggplot2)) install.packages("ggplot2")
if(!require(stringr)) install.packages("stringr")
if(!require(forcats)) install.packages("forcats")
if(!require(kableExtra)) install.packages("kableExtra")
if(!require(tidyr)) install.packages("tidyr")
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(anytime)) install.packages("anytime", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
if(!require(plyr)) install.packages("plyr")
if(!require(lubridate)) install.packages("lubridate")
if(!require(caTools)) install.packages("caTools")
if(!require(ggthemes)) install.packages("ggthemes")
if(!require(reshape2)) install.packages("reshape2")
if(!require(data.table)) install.packages("data.table")
if(!require(corrgram)) install.packages("corrgram")
if(!require(corrplot)) install.packages("corrplot")
if(!require(formattable)) install.packages("formattable")
if(!require(cowplot)) install.packages("cowplot")
if(!require(ggpubr)) install.packages("ggpubr")
if(!require(plotly)) install.packages("plotly")
if(!require(countrycode)) install.packages("countrycode")
if(!require(leaflet)) install.packages("leaflet")
if(!require(RColorBrewer)) install.packages("RColorBrewer")
if(!require(maps)) install.packages("maps")
if(!require(rworldmap)) install.packages("rworldmap")
if(!require(ggalt)) install.packages("ggalt")

# Loading all needed libraries
library(ggplot2)
library(dplyr)
library(tidyverse)
library(tidyr)
library(stringr)
library(forcats)
library(kableExtra)
library(caret)
library(anytime)
library(tidyverse)
library(lubridate)
library(plyr)
library(lubridate)
library(caTools)
library(ggthemes)
library(reshape2)
library(data.table)
library(corrgram)       
library(corrplot)
library(formattable)
library(cowplot)
library(ggpubr)
library(plotly)
library(countrycode)  # Gets country code 
library(leaflet)      # Interactive maps
library(rworldmap)
library(RColorBrewer) 
library(maps)
library(ggalt)



web <- "https://storage.googleapis.com/kaggle-data-sets/894/813759/bundle/archive.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1589735972&Signature=KmkZOIm4EjH%2BTnC%2FxvYalGbhKrdRBntnFJ2%2FoApTxajfp3MXGpd4qHUfMUruUGq2F%2FC2HhN11l6zIH%2B7Ifju72fn8U29BScOe0Mf2KZjT3XLNE7M1zFKb3gsoL%2BmwqDpmcx3Dk0S92oPeYAFW9DtACIA8hQ9mbXdhYBKWtf67i299qs77HkPfIThYT8CYcsricI1PSueGEh7njOme6OL9%2BpKwZZICJk6ByeVcCCZOtF5i7LrimWyEVLK%2BGXi%2FwGuRGvIbgstlzELcwsV6FQI0KxaaDUMP8c8ynqgmzkVdnybDZhgI8Sy7%2B%2FaFfGuTwhRi8Z0EEEhHu4FwZIKmwt3Nw%3D%3D&response-content-disposition=attachment%3B+filename%3Dworld-happiness.zip"


download.file(web, "myFile.zip", quiet=TRUE,mode="wb", method='curl')
unzip("myFile.zip")

file.remove("myFile.zip")
Happiness <- read.csv('2019.csv')
Happiness_2015 <- read.csv('2015.csv')


# Changing the name of columns
colnames (Happiness) <- c( "Happiness.Rank","Country", "Happiness.Score",
                           "Economy", "Social.Support" ,
                          "Life.Expectancy", "Freedom", "Generosity",
                          "Trust")


Happiness$Continent <- NA

Happiness$Continent[which(Happiness$Country %in% c("Israel", "United Arab Emirates", "Singapore", "Thailand", "Taiwan Province of China",
                                                   "Qatar", "Saudi Arabia", "Kuwait", "Bahrain", "Malaysia", "Uzbekistan", "Japan",
                                                   "South Korea", "Turkmenistan", "Kazakhstan", "Turkey", "Hong Kong S.A.R., China", "Philippines",
                                                   "Jordan", "China", "Pakistan", "Indonesia", "Azerbaijan", "Lebanon", "Vietnam",
                                                   "Tajikistan", "Bhutan", "Kyrgyzstan", "Nepal", "Mongolia", "Palestinian Territories",
                                                   "Iran", "Bangladesh", "Myanmar", "Iraq", "Sri Lanka", "Armenia", "India", "Georgia",
                                                   "Cambodia", "Afghanistan", "Yemen", "Syria"))] <- "Asia"
Happiness$Continent[which(Happiness$Country %in% c("Norway", "Denmark", "Iceland", "Switzerland", "Finland",
                                                   "Netherlands", "Sweden", "Austria", "Ireland", "Germany",
                                                   "Belgium", "Luxembourg", "United Kingdom", "Czech Republic",
                                                   "Malta", "France", "Spain", "Slovakia", "Poland", "Italy",
                                                   "Russia", "Lithuania", "Latvia", "Moldova", "Romania",
                                                   "Slovenia", "North Cyprus", "Cyprus", "Estonia", "Belarus",
                                                   "Serbia", "Hungary", "Croatia", "Kosovo", "Montenegro",
                                                   "Greece", "Portugal", "Bosnia and Herzegovina", "Macedonia",
                                                   "Bulgaria", "Albania", "Ukraine"))] <- "Europe"
Happiness$Continent[which(Happiness$Country %in% c("Canada", "Costa Rica", "United States", "Mexico",  
                                                   "Panama","Trinidad and Tobago", "El Salvador", "Belize", "Guatemala",
                                                   "Jamaica", "Nicaragua", "Dominican Republic", "Honduras",
                                                   "Haiti"))] <- "North America"
Happiness$Continent[which(Happiness$Country %in% c("Chile", "Brazil", "Argentina", "Uruguay",
                                                   "Colombia", "Ecuador", "Bolivia", "Peru",
                                                   "Paraguay", "Venezuela"))] <- "South America"
Happiness$Continent[which(Happiness$Country %in% c("New Zealand", "Australia"))] <- "Australia"
Happiness$Continent[which(is.na(Happiness$Continent))] <- "Africa"

# Changing Continent column to factor

Happiness$Continent <- as.factor(Happiness$Continent)

str(Happiness)



# Changing the name of columns
colnames (Happiness_2015) <- c("Country","Region", "Happiness.Rank", "Happiness.Score",
                               "Standard.Error", "Economy", "Family",
                               "Life.Expectancy", "Freedom", "Trust","Generosity",
                                "Dystopia.Residual")

Happiness_2015$Continent <- NA

Happiness_2015$Continent[which(Happiness_2015$Country %in% c("Israel", "United Arab Emirates", "Singapore", "Thailand", "Taiwan Province of China",
                                                   "Qatar", "Saudi Arabia", "Kuwait", "Bahrain", "Malaysia", "Uzbekistan", "Japan",
                                                   "South Korea", "Turkmenistan", "Kazakhstan", "Turkey", "Hong Kong S.A.R., China", "Philippines",
                                                   "Jordan", "China", "Pakistan", "Indonesia", "Azerbaijan", "Lebanon", "Vietnam",
                                                   "Tajikistan", "Bhutan", "Kyrgyzstan", "Nepal", "Mongolia", "Palestinian Territories",
                                                   "Iran", "Bangladesh", "Myanmar", "Iraq", "Sri Lanka", "Armenia", "India", "Georgia",
                                                   "Cambodia", "Afghanistan", "Yemen", "Syria"))] <- "Asia"
Happiness_2015$Continent[which(Happiness_2015$Country %in% c("Norway", "Denmark", "Iceland", "Switzerland", "Finland",
                                                   "Netherlands", "Sweden", "Austria", "Ireland", "Germany",
                                                   "Belgium", "Luxembourg", "United Kingdom", "Czech Republic",
                                                   "Malta", "France", "Spain", "Slovakia", "Poland", "Italy",
                                                   "Russia", "Lithuania", "Latvia", "Moldova", "Romania",
                                                   "Slovenia", "North Cyprus", "Cyprus", "Estonia", "Belarus",
                                                   "Serbia", "Hungary", "Croatia", "Kosovo", "Montenegro",
                                                   "Greece", "Portugal", "Bosnia and Herzegovina", "Macedonia",
                                                   "Bulgaria", "Albania", "Ukraine"))] <- "Europe"
Happiness_2015$Continent[which(Happiness_2015$Country %in% c("Canada", "Costa Rica", "United States", "Mexico",  
                                                   "Panama","Trinidad and Tobago", "El Salvador", "Belize", "Guatemala",
                                                   "Jamaica", "Nicaragua", "Dominican Republic", "Honduras",
                                                   "Haiti"))] <- "North America"
Happiness_2015$Continent[which(Happiness_2015$Country %in% c("Chile", "Brazil", "Argentina", "Uruguay",
                                                   "Colombia", "Ecuador", "Bolivia", "Peru",
                                                   "Paraguay", "Venezuela"))] <- "South America"
Happiness_2015$Continent[which(Happiness_2015$Country %in% c("New Zealand", "Australia"))] <- "Australia"
Happiness_2015$Continent[which(is.na(Happiness_2015$Continent))] <- "Africa"



# Changing Continent column to factor

Happiness_2015$Continent <- as.factor(Happiness_2015$Continent)

str(Happiness_2015)

```

# Introduction

Predictive machine learning algorithms are often at the forefront of modern computational science. Autonomously estimating results given troves of simulated data is one of the most powerful uses of machine learning.  Examples vary from determining exoplanet existence based on light flux data to leveraging the flow of the US bond and stock markets. Depending on the type of data you are processing, prediction can be accomplished through classification models, random forest, k-nearest neighbors or many other machine learning algorithms. This project employs regression analysis in order to study the happiness of countries.

## Overview

The World Happiness Report is a landmark survey of the state of global happiness. The first report was published in 2012, the second in 2013, the third in 2015, and the fourth in the 2016 Update. The World Happiness 2017, which ranks 155 countries by their happiness levels, was released at the United Nations at an event celebrating International Day of Happiness on March 20th. The report continues to gain global recognition as governments, organizations and civil society increasingly use happiness indicators to inform their policy-making decisions. Leading experts across fields – economics, psychology, survey analysis, national statistics, health, public policy and more – describe how measurements of well-being can be used effectively to assess the progress of nations. The reports review the state of happiness in the world today and show how the new science of happiness explains personal and national variations in happiness.

The [World Happiness Report](https://worldhappiness.report), uploaded to [Kaggle](https://www.kaggle.com/unsdsn/world-happiness) by the Sustainable Development Solutions Network, is described as "a landmark survey of the state of global happiness." The most recent survey was conducted throughout 2019 and ranks all countries with a happiness score. Particular factors of a society are included with the score that help gauge the livelihood in that country.


The rest of the report is organized as follows: second, the data set is dissected and visualized in the **Data** section. Third, the data is partitioned and the three modeling methods are shown in the **Methods** section. Fourth, the three models are compared in the **Results** section. Finally, the **Conclusion** section summarizes and compares the results.


## Data Exploration 

The data being used has 156 countries listed with 9 informative columns. These columns are happiness rank, country name, overall happiness score, and six factors that aid in defining the happiness score: GDP per capita, Social Support, Life Expectancy, Freedom, Generosity, and Truth.

The scores are based on answers from the Gallup World Poll to the main life evaluation question asked in the poll. This question, known as the *Cantril ladder*, asks respondents to think of a ladder with the best possible life for them being a 10 and the worst possible life being a 0 and to rate their own current lives on that scale. The emblematic Orwellian dystopia, a society in which the above aspects are destructive to the welfare of a free and open society, is most closely associated with the lowest scoring countries. Therefore, higher scores represent, essentially, a closer approximation to a Utopia.


Confusion exists as to how the numbers for each factor apply to the happiness score (discussions can be found (https://www.kaggle.com/unsdsn/world-happiness/discussion/116054) and (https://www.kaggle.com/unsdsn/world-happiness/discussion/35141)). These values represent the effect that each societial variable adds to a standard dystopian score. Similarly, having higher factor scores will lead to a higher happiness score. It is worth mentioning that previous versions of the happiness report have a dystopia residual that represent more general improvements to the standard dystopian. This is not available in the most recent (2018, 2019) data sets.

It is important to note that the factor values themselves are not *directly* correlated with the overall happiness score. The goal of this report is to determine the extent in which they are correlated. The previously mentioned discussions state the best model one can use to estimate happiness scores is the sum of the individual factors, that is `sum(data$[factors]) = data$Score`. This model is presented along with a generalized linear model equation based on 2019 data alone and 2018/2019 data together.

# Analysis
## Visualization
In this section, we will play with different variables to find out how they correlate with each other.

The qualitative structure and the general relationship of the data is described above. The data found here is quantitatively normal. A histogram of the happiness scores shows an approximately normal distribution and descriptive statistics support this claim.

```{r hist_score, message = F, warning = F, echo=FALSE}
hist(Happiness$Happiness.Score, freq=TRUE, col="black", border="white", 
     main="2019 Happiness Scores", xlab="Happiness.Score", ylab="Count")
```


```{r message = F, warning = F, echo=FALSE}
Happiness %>%
  ggplot(aes(Happiness.Score)) +
  geom_density(alpha = 0.2, bw = 0.75, position = "stack")
```


The histogram is right-leaning. This can be shown quantitively by comparing the mean score (`r mean(Happiness$Happiness.Score)`) and the median score (`r median(Happiness$Happiness.Score)`). A mean greater than the median signifies that there are more values larger that the middle. The range of scores is (2.853, 7.769). The highest and lowest observations are displayed below: 


```{r message = F, warning = F, echo=FALSE}
d15<-Happiness_2015 %>% select(Country,Continent,HS15=Happiness.Score)
d19<-Happiness %>% select(Country,Continent,HS19=Happiness.Score)
score<-inner_join(d15,d19)%>% mutate(score_diff= HS19-HS15)%>% filter(score_diff>0)
score$Country <- factor(score$Country, levels=as.character(score$Country))
gg <- ggplot(score, aes(x=HS15, xend=HS19, y=Country, group=Country)) + 
  geom_dumbbell(size=2, color="#e3e2e1", 
                colour_x = "#5b8124", colour_xend = "#bad744",
                dot_guide=TRUE, dot_guide_size=0.25) + 
  labs(x=NULL, 
       y=NULL, 
       
       title=" Country Happiness Scores Increased: 2015 vs 2019"
  ) +
  theme(plot.title = element_text(hjust=0.5, face="bold"),
        plot.background=element_rect(fill="#f7f7f7"),
        panel.background=element_rect(fill="#f7f7f7"),
        panel.grid.minor=element_blank(),
        panel.grid.major.y=element_blank(),
        panel.grid.major.x=element_line(),
        axis.ticks=element_blank(),
        legend.position="top",
        panel.border=element_blank())
plot(gg)

```


```{r message = F, warning = F, echo=FALSE , fig.cap = "My plot", dev = "png"}
q<-map_data("world")
colnames(q)[5] <- "Country"
df<- left_join(q,Happiness)

ggplot()  +
  geom_polygon( aes(x = df$long, y = df$lat, group = df$group,fill= df$Happiness.Score)) + 
  coord_equal() +scale_fill_gradient(breaks=c(3,5,7,9)) +
  ggtitle("Happiness score ")+
  xlab("") + ylab("") + guides(shape=FALSE) + labs(fill="Happiness Score")

ggplot()  +
  geom_polygon( aes(x = df$long, y = df$lat, group = df$group,fill= df$Freedom)) + 
  coord_equal() +scale_fill_gradient( low = "white", high = "black") +
  ggtitle("Freedom ")+
  xlab("") + ylab("") + guides(shape=FALSE) + labs(fill="Freedom")

ggplot()  +
  geom_polygon( aes(x = df$long, y = df$lat, group = df$group,fill= df$Generosity)) + 
  coord_equal() +scale_fill_gradient( low = "white", high = "red") +
  ggtitle("Generosity")+
  xlab("") + ylab("") + guides(shape=FALSE) + labs(fill="Generosity")

ggplot()  +
  geom_polygon( aes(x = df$long, y = df$lat, group = df$group,fill= df$Trust)) + 
  coord_equal() +scale_fill_gradient( low = "white", high = " dark green") +
  ggtitle("Trust")+
  xlab("") + ylab("") + guides(shape=FALSE) + labs(fill="Trust")

ggplot()  +
  geom_polygon( aes(x = df$long, y = df$lat, group = df$group,fill= df$Economy)) + 
  coord_equal() +scale_fill_gradient( low = "white", high = "brown") +
  ggtitle("Economy")+
  xlab("") + ylab("") + guides(shape=FALSE) + labs(fill="Economy")

ggplot()  +
  geom_polygon( aes(x = df$long, y = df$lat, group = df$group,fill= df$Life.Expectancy)) + 
  coord_equal() +scale_fill_gradient( low = "white", high = "#800080") +
  ggtitle("Life.Expectancy")+
  xlab("") + ylab("") + guides(shape=FALSE) + labs(fill="Life.Expectancy")

```


```{r message = F, warning = F, echo=FALSE}

#table with average happiness per region
avg_happiness_region <-Happiness %>%
  group_by(Continent) %>%          
  summarise(avg_happiness = mean(Happiness.Score, round(1)))


#Plotting the average happiness scores to compare regions
ggplot(avg_happiness_region, aes(y=avg_happiness, x=Continent)) + 
  geom_bar( stat="identity") + theme_bw() + geom_line(aes(y = mean(Happiness$Happiness.Score)), size = 1.5, color="red", group = 1 ,name = "world av")+
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Average value of happiness variables for different continents", 
       y = "Average value") 
```


Top 3 happiest regions based on average happiness score are: Australia & North America & Western Europe
It is important to note that both of the first two regions include only 2 countries and  Europe has 21 countries. Additionally all of these regions include countries with developed economies.

The unhappiest region is Africa, which includes 40 different countries. The second unhappiest region is Asia.



## Correlation plot
Let's see the correlation between numerical variables in our dataset.



```{r, message = F, warning = F , echo=FALSE}
########## Correlation between variables

# Finding the correlation between numerical columns
Num.cols <- sapply(Happiness, is.numeric)
Cor.data <- cor(Happiness[, Num.cols])

corrplot(Cor.data, method = 'color')  
  
```

Obviously, there is an inverse correlation between "Happiness Rank"  and all the other
numerical variables. In other words, the lower the happiness rank, the higher the happiness score, and the higher the other seven factors that contribute to happiness.
So let's remove the happiness rank, and see the correlation again.



## Comparing different continents regarding their happiness variables
Let's calculate the average happiness score and the average of the other seven variables for each continent. Then melt it to have variables and values in separate columns. Finally, using ggplot to show the difference between continents.


```{r, message = F, warning = F, echo=FALSE}
## Comparing different continents regarding their happiness variables
Happiness.Continent <- Happiness %>%
  group_by(Continent) %>%
  summarise(Economy= mean(Economy), Social.Support = mean(Social.Support),
               Life.Expectancy = mean(Life.Expectancy), Freedom = mean(Freedom),
               Generosity = mean(Generosity), Trust = mean(Trust))


Happiness.Continent.melt <- melt(Happiness.Continent)

# Faceting
ggplot(Happiness.Continent.melt, aes(y=value, x=Continent, color=Continent, fill=Continent)) + 
  geom_bar( stat="identity") +    
  facet_wrap(~variable) + theme_bw() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Average value of happiness variables for different continents", 
       y = "Average value") 
  
```

We can see that Australia has approximately the highest average in all fields except dystopia residual, after that Europe, North America, and South America are roughly the same regarding happiness score and the other seven factors. Finally, Asia and Africa have the lowest scores in all fields.

## Correlation plot for each continent  
Let's see the correlation between variables for each continent.


```{r, message = F, warning = F, echo=FALSE}
# Create a correlation plot
corrgram(Happiness %>% select(-3) %>% filter(Continent == "Africa"), order=TRUE,
         upper.panel=panel.cor, main="Happiness Matrix for Africa")
```

**Correlation between "Happiness Score" and the other variables in Africa:**  
Economy  > Life.Expectancy > Freedom  
There is no correlation between happiness score and trust.  
There is an inverse correlation between happiness score and generosity.


```{r, message = F, warning = F, echo=FALSE}
# Create a correlation plot
corrgram(Happiness %>% select(-3) %>% filter(Continent == "Asia"), order=TRUE,
         upper.panel=panel.cor, main="Happiness Matrix for Asia")
```

**Correlation between "Happiness Score" and the other variables in Asia:**  
Economy > Life.Expectancy > Freedom > Trust 
There is no correlation between happiness score and generosity.

```{r, message = F, warning = F, echo=FALSE}
# Create a correlation plot
corrgram(Happiness %>% select(-3) %>% filter(Continent == "Europe"), order=TRUE,
         upper.panel=panel.cor, main="Happiness Matrix for Europe")
```

**Correlation between "Happiness Score" and the other variables in Europe:**  
Freedom > Trust > Economy > Life.Expectancy > Generosity  
The highest correlation between generosity and happiness score took place in Europe.



```{r, message = F, warning = F, echo=FALSE}
# Create a correlation plot
corrgram(Happiness %>% select(-3) %>% filter(Continent == "North America"), order=TRUE,
         upper.panel=panel.cor, main="Happiness Matrix for North America")
```

**Correlation between "Happiness Score" and the other variables in North America:**  
Life.Expectancy > Economy > Freedom > Trust  
There is an inverse correlation between happiness score and generosity.



```{r, message = F, warning = F, echo=FALSE}
# Create a correlation plot
corrgram(Happiness %>% select(-3) %>% filter(Continent == "South America"), order=TRUE,
         upper.panel=panel.cor, main="Happiness Matrix for South America")
```


**Correlation between "Happiness Score" and the other variables in South America:**  
Dystopia.Residual > Economy > Life.Expectancy > Freedom > Generosity > Trust > Family  
The family is the least significant factor in South America.  

## Happiness score comparison on different continents  
We will use scatter plot, box plot, and violin plot to see the happiness score distribution in different countries, how this score is populated in these continents and also will calculate the mean and median of happiness score for each of these continents.


```{r, message = F, warning = F, echo=FALSE}
## Happiness score comparison on different continents
####### Happiness score for each continent

gg1 <- ggplot(Happiness,
              aes(x=Continent,
                  y=Happiness.Score,
                  color=Continent))+
  geom_point() + theme_bw() +
  theme(axis.title = element_text(family = "Helvetica", size = (8)))

gg2 <- ggplot(Happiness , aes(x = Continent, y = Happiness.Score)) +
  geom_boxplot(aes(fill=Continent)) + theme_bw() +
  theme(axis.title = element_text(family = "Helvetica", size = (8)))

gg3 <- ggplot(Happiness,aes(x=Continent,y=Happiness.Score))+
  geom_violin(aes(fill=Continent),alpha=0.7)+ theme_bw() +
  theme(axis.title = element_text(family = "Helvetica", size = (8)))

# Compute descriptive statistics by groups
stable <- desc_statby(Happiness, measure.var = "Happiness.Score",
                      grps = "Continent")
stable <- stable[, c("Continent","mean","median")]
names(stable) <- c("Continent", "Mean of happiness score","Median of happiness score")
# Summary table plot
stable.p <- ggtexttable(stable,rows = NULL, 
                        theme = ttheme("classic"))


ggarrange(gg1, gg2, ncol = 1, nrow = 2)
```


As we have seen before, Australia has the highest median happiness score. Europe, South America, and North America are in the second place regarding median happiness score. Asia has the lowest median after Africa. We can see the range of happiness score for different continents, and also the concentration of happiness score.  

## Scatter plot with regression line  
Let's see the correlation between happiness score and the other seven factors in the happiness dataset for different continents by creating a scatter plot.


```{r, message = F, warning = F, echo=FALSE}
## Scatter plot with regression line  
ggplot(subset(Happiness, Happiness$Continent != "Australia"), aes(x = Life.Expectancy, y = Happiness.Score)) + 
  geom_point(aes(color=Continent), size = 3, alpha = 0.8) +  
  geom_smooth(aes(color = Continent, fill = Continent), 
              method = "lm", fullrange = TRUE) +
  facet_wrap(~Continent) +
  theme_bw() + labs(title = "Scatter plot with regression line")
```

The correlation between life expectancy and happiness score in Europe, North America, and Asia is more significant than the other continents. Worth mentioning that we will not take Australia into account because there are just two countries in Australia and creating scatter plot with the regression line for this continent will not give us any insight.


```{r, message = F, warning = F, echo=FALSE}
ggplot(subset(Happiness, Happiness$Continent != "Australia"), aes(x = Economy, y = Happiness.Score)) + 
  geom_point(aes(color=Continent), size = 3, alpha = 0.8) +  
  geom_smooth(aes(color = Continent, fill = Continent), 
              method = "lm", fullrange = TRUE) +
  facet_wrap(~Continent) +
  theme_bw() + labs(title = "Scatter plot with regression line")
```


We can see pretty the same result here for the correlation between happiness score and economy. Africa has the lowest relationship in this regard.


```{r, message = F, warning = F, echo=FALSE}
## Scatter plot with regression line  
ggplot(subset(Happiness, Happiness$Continent != "Australia"), aes(x = Freedom, y = Happiness.Score)) + 
  geom_point(aes(color=Continent), size = 3, alpha = 0.8) +  
  geom_smooth(aes(color = Continent, fill = Continent), 
              method = "lm", fullrange = TRUE) +
  facet_wrap(~Continent) +
  theme_bw() + labs(title = "Scatter plot with regression line")
```


Freedom in Europe and North America is more correlated to happiness score than any other continents.


```{r, message = F, warning = F, echo=FALSE}
## Scatter plot with regression line  
ggplot(subset(Happiness, Happiness$Continent != "Australia"), aes(x = Trust, y = Happiness.Score)) + 
  geom_point(aes(color=Continent), size = 3, alpha = 0.8) +  
  geom_smooth(aes(color = Continent, fill = Continent), 
              method = "lm", fullrange = TRUE) +
  facet_wrap(~Continent) +
  theme_bw() + labs(title = "Scatter plot with regression line")
```

Approximately there is no correlation between trust and happiness score in Africa.


```{r, message = F, warning = F, echo=FALSE}
## Scatter plot with regression line  
ggplot(subset(Happiness, Happiness$Continent != "Australia"), aes(x = Generosity, y = Happiness.Score)) + 
  geom_point(aes(color=Continent), size = 3, alpha = 0.8) +  
  geom_smooth(aes(color = Continent, fill = Continent), 
              method = "lm", fullrange = TRUE) +
  facet_wrap(~Continent) +
  theme_bw() + labs(title = "Scatter plot with regression line")
```


The regression line has a positive slope only for Europe and South America. For Asia the line is horizontal, and for Africa and North America the slope is negative.


## Scatter plot colored by Continents  

The following is just another way of seeing happiness score distribution on different continents when taking the correlation of happiness score with different variables into account.


```{r message = F, warning = F, echo=FALSE}
ggplotRegression <- function (fit) {
  
  
  
  ggplot(fit$model, aes_string(x = names(fit$model)[2], y = names(fit$model)[1])) + 
    geom_point(shape=1,size=3,color="#003399")+
    stat_smooth(method = "lm", col = "red") +
    labs(title = paste("R2 = ",signif(summary(fit)$r.squared, 5),
                       "Intercept =",signif(fit$coef[[1]],5 ),
                       " Slope =",signif(fit$coef[[2]], 5),
                       " P =",signif(summary(fit)$coef[2,4], 5)))
}


gg2<-ggplotRegression(lm(Happiness.Score ~ Generosity, data = Happiness))
gg3<-ggplotRegression(lm(Happiness.Score ~ Freedom, data = Happiness))
gg4<-ggplotRegression(lm(Happiness.Score ~ Economy, data = Happiness))

gg6<-ggplotRegression(lm(Happiness.Score ~ Life.Expectancy, data = Happiness))
gg7<-ggplotRegression(lm(Happiness.Score ~ Trust, data = Happiness))

ggarrange(gg2,gg3,ncol=1,nrow=2)
ggarrange(gg4,gg6,ncol=1,nrow=2)
ggarrange(gg7,ncol=1,nrow=2)


```

Visualizing data patterns may offer clues of intriguing routes to choose when performing analysis but it doesn't provide any noteworthy prediction capabilities. This is where machine learning techniques begin to excel. In the next section, we build several models to determine the best way to predict a happiness score.



# Methods

## Model 1: The Sum of Factors

The first model is seemingly the most obvious and was proposed on a [discussion forum](https://www.kaggle.com/unsdsn/world-happiness/discussion/35141) from the Kaggle website. It states that the "perfect" prediction model is to simply take the sum of all factors as the happiness score. This model is attempted with one caveat: a "standard dystopia score" was discovered in earlier happiness reports and was given the value 1.85. This value is added to our predicted scores as well because each factor is a ranking of how much *better* the country is than the standard dystopia. Note the use Root Mean Square Error (RMSE) as a success indicator. This choice is further explained in the results section. 

```{r message = F, warning = F, echo=FALSE}
# find  predicted score by sum method and calculate the corresponding RMSE
data <- read.csv('2019.csv')
sum_model <- data %>% mutate(pred_score = GDP.per.capita +
                               Social.support +
                               Healthy.life.expectancy +
                               Freedom.to.make.life.choices + 
                               Generosity + 
                               Perceptions.of.corruption + 
                               1.85, 
                             RMSE = RMSE(Score, pred_score))

# show top results of the summation model
sum_model %>%
  filter(Overall.rank <= 5) %>%
  select(Overall.rank, Country.or.region, Score, pred_score, RMSE)
```

```{r message = F, warning = F, echo=FALSE}
# save RMSE for the first model
mod1_rmse <- RMSE(sum_model$Score, sum_model$pred_score)
```

All predicted scores shows the same RMSE! In addition to the dystopian standard score, previous Happiness Report data sets also have a "dystopian residual" that contributes to the happiness score. Since the residual is not shown or described in this data set, it is deemed inappropriate to introduce it ourselved into the model. Although, without this value, the data set does seem incomplete. In spite of its absence, the dystopian residual column is calculated and a snippet of these are shown below. This is calculated under the assumption the summation model is perfect. Simply subtracting all factors and the standard dystopia value from the happiness score will yield the residual. There is no mention of the missing residual in any recent discussions to the 2018 or 2019 dataset, therefore it is omitted in subsequent modeling techniques. We can conclude that the sum of factors model is not perfectly accurate as portrayed in the discussion boards; it has a RMSE of 0.5280065.

```{r message = F, warning = F, echo=FALSE}
# calculate the missing dystopian residuals
sum_model <- sum_model %>% mutate(residual = Score - pred_score)

# show top results of the summation model
sum_model %>%
  filter(Overall.rank <= 5) %>%
  select(Overall.rank, Country.or.region, Score, pred_score, RMSE, residual)
```


## Model 2: The 2019 GLM Model

Before our first linear regression model is applied, the data must be partitioned into a training and test set. This step is common when employing machine learning algorithms that require a check on the goodness of fit. It reduces the probability of overfitting to our training data at the expense of our prediction model. This was not completed for our sum of factors model because a model did not need to be trained, the equation was simply `sum(data$[factors])`.

The Happiness Report has a little over 150 country observations and six factors in which we will condition our model. Given the relatively low number of observations compared to the amount of factors, the model may have a tendency to overfit to the training data by overweighting unimportant variables. This is almost unavoidable when working with low volumes of data. The reality of regression is that you can *always* find a model that fits your training data exactly but which is typically useless for prediction. Keeping this in mind, an optimal training-test data split ratio, that is 70 training:30 test, 80 training:20 test, ..., etc., is first determined.

```{r message = F, warning = F, echo=FALSE}
# --- test for an appropriate ratio in data partitioning
# a sequence of p's we want to test
ps <- seq(from=.30, to=.90, by=.01)

# calculate RMSEs for each p value
rmses <- sapply(ps, function(p){
  train_index <- createDataPartition(data$Score, times=1, p=p, list=FALSE)
  train <- data[train_index,]
  test <- data[-train_index,]
  fit <- glm(Score ~ GDP.per.capita +
               Social.support +
               Healthy.life.expectancy + 
               Freedom.to.make.life.choices + 
               Generosity + 
               Perceptions.of.corruption, 
             data = train)
  test <- test %>% mutate(pred_score = predict.glm(fit, newdata=test))
  RMSE(test$Score, test$pred_score)
})
```

The tested model in the partitioning is explored after this section. Plotting `rmses` versus `p` shows a slight pattern: when you increase the training data size, our RMSE decreases. Intuitively, this makes sense. The data is quite correlated as it has already been shown and the model is being allowed to work with more training data to make better predictions in the test set. From the plot below, the lowest RMSE is `r min(rmses)` with a ratio of `r ps[which.min(rmses)]`:`r 1-ps[which.min(rmses)]`.

```{r message = F, warning = F, echo=FALSE}
# no real clear winner in terms of best accuracy in probabilities
plot(ps, rmses)
```

While useful in achieving a low RMSE, employing only `r 1-ps[which.min(rmses)]` percent of our data to test does not leave much in terms of prediction. Ultimately, an arbitrary value of 0.70 is chosen because RMSE seems to become more sporadic after this value. Future models in the *methods* section use 0.70 as well. This ratio is also kept when the current data is supplemented with more data.

```{r message = F, warning = F, echo=FALSE}
# set seed to keep partitioning consistent
set.seed(1, sample.kind = "Rounding")

# ----- Data partitioning -----
train_index <- createDataPartition(data$Score, times=1, p=0.70, list=FALSE)
train <- data[train_index,]
test <- data[-train_index,]
```

With our data partitioned 0.70:0.30, a generalized linear model is fitted using the `caret` package. `Score` is predicted using all six factors. The first five predicted scores are shown.

```{r message = F, warning = F, echo=FALSE}
# --- fit our glm model, caret::glm
fit <- glm(Score ~ GDP.per.capita +
             Social.support +
             Healthy.life.expectancy + 
             Freedom.to.make.life.choices + 
             Generosity + 
             Perceptions.of.corruption, 
           data = train)

# add predicted scores to a 'results' data frame
results <- test %>% 
  mutate(pred_score = predict.glm(fit, newdata=test))
```

```{r message = F, warning = F, echo=FALSE}
# show top five observations
results %>%
  select(Overall.rank, Country.or.region, Score, pred_score) %>%
  head()

# show bottom five observations
results %>%
  select(Overall.rank, Country.or.region, Score, pred_score) %>%
  tail()
```

The top five and bottom five observations are shown above compared with the actual score. The `results` data frame is plotted below with a line of best fit in blue and a reference line in red at `y = x`. If the model was to work perfectly, the line of best fit would follow the reference line because each predicted score would be equal to the score.

```{r message = F, warning = F, echo=FALSE}
# plot predicted scores vs actual scores
# also plot y = x line
ggplot(data = results, aes(Score, pred_score)) + 
  geom_point(color='black') +
  geom_smooth(method = "lm", se = TRUE) +
  geom_abline(color='red')
```

Even though results have been shown, it is worth mentioning the use of RMSE instead of other success indicators. RMSE suggests how close (or far) your predicted values are from the actual data you are attempting to model. The use of a success measure for this model, and others in the methods section, is to understand the accuracy and precision of the model's predictions. For this reason, RMSE is used as a success metric over alternatives. The RMSE of this model is `r RMSE(results$Score, results$pred_score)`. The coefficients of the fitted model are shown below for completeness:

```{r message = F, warning = F, echo=FALSE}
# save model 2 RMSE
mod2_rmse_gen <- RMSE(results$Score, results$pred_score)

# save coefficients to use in equation
c <- coefficients(fit)
c[] <- lapply(c, round, 3)

# print coefficients of fitted model
fit$coefficients
```

The following equation is the final model equation. Using these coefficients and the following notation predicted score = $\hat{y}$, GDP per capita score = $x_{GDP}$, Social Support score = $x_{SS}$, Life Expectancy score = $x_{HEA}$, Freedom score = $x_{FRE}$, Generosity score = $x_{GEN}$, Truth score = $x_{TRU}$.

\begin{equation}
  \hat{y} = `r c[1]` + `r c[2]` x_{GDP} + `r c[3]` x_{SS} + `r c[4]` x_{HEA} + `r c[5]` x_{FRE} + `r c[6]` x_{GEN} + `r c[7]` x_{TRU} 
\end{equation}



### Removing `Generosity`

In an attempt to improve the model, we can remove the generosity component because early evaluations pointed to it being the least correlated factor. The previously partitioned data (`p = 0.70`) is used in this model as well.

```{r message = F, warning = F, echo=FALSE}
# fit model without generosity
fit <- glm(Score ~ GDP.per.capita +
             Social.support +
             Healthy.life.expectancy + 
             Freedom.to.make.life.choices + 
             Perceptions.of.corruption, 
           data = train)

# add predicted scores to a 'results' data frame
results <- test %>% 
  mutate(pred_score = predict.glm(fit, newdata=test))

# plot predicted scores vs actual scores
ggplot(data = results, aes(Score, pred_score)) + 
  geom_point(color='black') +
  geom_smooth(method = "lm", se = TRUE) +
  geom_abline(color='red')
```

This model yields a RMSE of `r RMSE(results$Score, results$pred_score)`. The model coefficients are shown below along with the model equation following the same format as before.

```{r message = F, warning = F, echo=FALSE}
# save rmse and coefficients
mod2_rmse_nogen <- RMSE(results$Score, results$pred_score)
c <- coefficients(fit)
c[] <- lapply(c, round, 3)

# print coefficients of fitted model
fit$coefficients
```

\begin{equation}
  \hat{y} = `r c[1]` + `r c[2]` x_{GDP} + `r c[3]` x_{SS} + `r c[4]` x_{HEA} + `r c[5]` x_{FRE} + `r c[6]` x_{TRU} 
\end{equation}



## Model 3: The 2018/19 GLM Model

In an ideal world, data sets would be infintely large and models can be fitted with as much precision as your CPU can handle. The next model uses data from the 2018 and 2019 surveys. While not exactly an infinite data set, doubling the size of our train and test sets do allow for more training and testing. Combining data from old surveys is appropriate in this scenario because these two years follow the same grading scheme, specifically they both do not have the dystopian residuals mentioned above. The first step is to reproduce new train and test sets, then run a model including all factors.

```{r message = F, warning = F, echo=FALSE}
# add 2018 data set and keep only used columns
data18 <- read.csv("2018.csv")
data18$Overall.rank <- NULL
data18$Country.or.region <- NULL

# remove unused columns in 2019 data frame for merging
data$Overall.rank <- NULL
data$Country.or.region <- NULL

# turn corruption column from factor to numeric, turn NAs to 0
data18$Perceptions.of.corruption <- as.numeric(as.character(data18$Perceptions.of.corruption))
data18[is.na(data18)] <- 0

# build full data set of both 2019, 2018 data
full_data <- rbind(data, data18)

# show full data set
glimpse(full_data)
```

Columns with unused data, such as country name and rank, were removed before merging. NAs are introduced by coercion when converting the corruption column of the 2018 data set from factor to numeric. These are changed to zeros and represent unfound data. A glimpse of the 2018/2019 data set is outputted above.

```{r message = F, warning = F, echo=FALSE}
# set seed to keep partitioning consistent
set.seed(1, sample.kind = "Rounding")

# partition data
train_index <- createDataPartition(full_data$Score, times=1, p=0.70, list=FALSE)
train <- full_data[train_index,]
test <- full_data[-train_index,]

# fit a model (including generosity)
fit <- glm(Score ~ GDP.per.capita +
             Social.support +
             Healthy.life.expectancy + 
             Freedom.to.make.life.choices + 
             Generosity +
             Perceptions.of.corruption, 
           data = train)

# add predicted scores to our 'results' data frame
results <- test %>% 
  mutate(pred_score = predict.glm(fit, newdata=test))

# plot predicted scores vs actual scores
ggplot(data = results, aes(Score, pred_score)) + 
  geom_point(color='black') +
  geom_smooth(method = "lm", se = TRUE) +
  geom_abline(color='red')
```

We see a better fitting model here, especially around the median, where the confidence bands are small. This model yields a RMSE of `r RMSE(results$Score, results$pred_score)`. The model coefficients are shown below along with the model equation following the same notation as before.

```{r message = F, warning = F, echo=FALSE}
# save rmse and coefficients
mod3_rmse_gen <- RMSE(results$Score, results$pred_score)
c <- coefficients(fit)
c[] <- lapply(c, round, 3)


# print coefficients of fitted model
fit$coefficients
```

\begin{equation}
  \hat{y} = `r c[1]` + `r c[2]` x_{GDP} + `r c[3]` x_{SS} + `r c[4]` x_{HEA} + `r c[5]` x_{FRE} + `r c[6]` x_{GEN} + `r c[7]` x_{TRU} 
\end{equation}



### Removing `Generosity`

For curiosity's sake, the previous model may be improved by removing the somewhat uncorrelated generosity factor seen in earlier sections. This is completed with the 2018-2019 data set, `full_data`.

```{r message = F, warning = F, echo=FALSE}
# fit a model (not including generosity)
fit <- glm(Score ~ GDP.per.capita +
             Social.support +
             Healthy.life.expectancy + 
             Freedom.to.make.life.choices + 
             Perceptions.of.corruption, 
           data = train)

# add predicted scores to our 'results' data frame
results <- test %>% 
  mutate(pred_score = predict.glm(fit, newdata=test))

# plot predicted scores vs actual scores
ggplot(data = results, aes(Score, pred_score)) + 
  geom_point(color='black') +
  geom_smooth(method = "lm", se = TRUE) +
  geom_abline(color='red')
```

We see a better fitting model here, especially around the median, where the confidence bands are small. This model yields a RMSE of `r RMSE(results$Score, results$pred_score)`. The model coefficients are shown below along with the model equation following the same notation as before.

```{r message = F, warning = F, echo=FALSE}
# save rmse and coefficients
mod3_rmse_nogen <- RMSE(results$Score, results$pred_score)
c <- coefficients(fit)
c[] <- lapply(c, round, 3)

# print coefficients of fitted model
fit$coefficients
```

\begin{equation}
  \hat{y} = `r c[1]` + `r c[2]` x_{GDP} + `r c[3]` x_{SS} + `r c[4]` x_{HEA} + `r c[5]` x_{FRE} + `r c[6]` x_{TRU} 
\end{equation}



# Results

The results of our five models are shown in the table below. It is clearly shown that the best model in terms of RMSE is the sum of factors model. Even though the data seemed incomplete with dystopian residuals, taking the sum of the factor and the standard dystopian scores yield the closest score predictions.

```{r message = F, warning = F, echo=FALSE}
# compiles the list of RMSEs
rmse_list <- list(mod1_rmse,
                  mod2_rmse_gen,
                  mod2_rmse_nogen,
                  mod3_rmse_gen,
                  mod3_rmse_nogen)
rmse_list[] <- lapply(rmse_list, round, 3)
```

| Method                              | RMSE              |
|-------------------------------------|-------------------|
| Sum of Factors Model                | `r rmse_list[1]`  |
| GLM Model 2019                      | `r rmse_list[2]`  |
| GLM Model 2018/2019                 | `r rmse_list[4]`  |
| GLM Model - No Generosity 2019      | `r rmse_list[3]`  |
| GLM Model - No Generosity 2018/2019 | `r rmse_list[5]`  |

For completeness, it may be useful to check the summation model with the full 2018-2019 data set. This is shown below and reported in the final table.

```{r message = F, warning = F, echo=FALSE}
# find our predicted score and calculate the corresponding RMSE
sum_model <- full_data %>% mutate(pred_score = GDP.per.capita +
                                    Social.support +
                                    Healthy.life.expectancy +
                                    Freedom.to.make.life.choices + 
                                    Generosity + 
                                    Perceptions.of.corruption + 
                                    1.85, 
                                  RMSE = RMSE(Score, pred_score))

# save RMSE for the first model
mod1_rmse_full <- RMSE(sum_model$Score, sum_model$pred_score)
```

```{r message = F, warning = F, echo=FALSE}
# recompiles the list of RMSEs
rmse_list <- list(mod1_rmse,
                  mod2_rmse_gen,
                  mod2_rmse_nogen,
                  mod3_rmse_gen,
                  mod3_rmse_nogen,
                  mod1_rmse_full)
rmse_list[] <- lapply(rmse_list, round, 3)
```

It is clearly seen that the best model is the summation model. Interestingly the GLM models do not improve when the data set size doubles. This may be due to overfitting in smaller data size that does not occur in the larger data.

| Method                              | RMSE              |
|-------------------------------------|-------------------|
| Sum of Factors Model                | `r rmse_list[1]`  |
| Sum of Factors Model 2018/2019      | `r rmse_list[6]`  |
| GLM Model 2019                      | `r rmse_list[2]`  |
| GLM Model 2018/2019                 | `r rmse_list[4]`  |
| GLM Model - No Generosity 2019      | `r rmse_list[3]`  |
| GLM Model - No Generosity 2018/2019 | `r rmse_list[5]`  |



# Conclusion

Machine learning algorithms have become a paramount tool in computer science and analysis. This report explores this use with regression analysis on a dataset of national happiness. Results are found that are rather unexpected: a glaring error in the data did not impede the simplest model, increasing training data set size did not decrease the error, and reducing the effectiveness of uncorrelated variables did not decrease the error.

This report felt limited in the amount of data being trained and tested. If more consistent data was available, it is my belief that the summation model would out perform the GLM models. The omitted residuals left a hole in the data that may have provided more insight into the validity of the summation model. The discussion boards were helpful in this respect by providing insight on the purpose of the data. Further work on a project such as this would include calculating and using dystopian residuals for data sets where they were lost, combining previous years reports, and comparing more adaptive models on the large data sets.











